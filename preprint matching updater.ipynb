{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outbreak resource litcovid and preprint matcher\n",
    "\n",
    "This code pings the outbreak.info api to pull an updated list of ids, compares the ids with files containing previously run ids and identifies the newly updated ids. For the newly updated ids, it pings the api to pull the relevant metadata so that a similarity test can be run for the new ids.\n",
    "\n",
    "**Requirements**\n",
    "This code was written in python 3.6 and uses the following libraries:\n",
    "* requests\n",
    "* pickle\n",
    "* json\n",
    "* pandas\n",
    "* nltk\n",
    "* string\n",
    "* datetime\n",
    "\n",
    "**Limitations**\n",
    "This code does not account for publications hosted in Zenodo, Dataverse, Figshare, or any other general repository, as the relationship between publications hosted on those sites and litcovid publications cannot be automatically determined.  This code is only for linking preprints in biorxiv and medrxiv to litcovid. Note that it currently does not accommodate preprint rxivs outside of biorxiv and medrxiv as the parsers for those preprints have yet to be written.\n",
    "\n",
    "**Assumptions**\n",
    "In order to minimize manual review, the thresholds have been set pretty high so precision is expected to be high, but sensitivity is expected to be low. An initial run was already performed and all the relevant data was already saved.  This data is included in the repo as detailed below\n",
    "\n",
    "**File structure**\n",
    "Previous results are 'cached' (ie-saved and updated), so that recalculations are not required, and time isn't wasted re-running\n",
    "Files may be named by type of meta compared (either 'text' or 'auth' (author)), and source (either 'litcovid' or 'preprint')\n",
    "\n",
    "**file paths**:\n",
    "* 'results/archives/' - stores precomputed files from previous runs and lists of identifiers in previous runs\n",
    "* 'temp/' - temporarily stores the type-specific successful matches in a run\n",
    "* 'to review/' - stores the results of the matching that require manual review\n",
    "* 'update dumps/' - stores the dataframe of updates to make based on sorted matches in this run\n",
    "\n",
    "**Pre-existing files**\n",
    "* 'results/archives/all_`source`_ids.txt' - a pickled list of identifiers that has already been run (where `source` is either litcovid or preprints\n",
    "* 'results/archives/`compare_type`_`source`_set.txt' - a pickled pandas dataframe containing preprocessed text for comparison. The `source` is again either litcovid or preprints, while the `compare_type` is either auth (author), or text (title and abstract)\n",
    "* 'temp/`compare_type`_above_threshold.txt - a tab-delimited text file containing all matches based on the `compare_type` (either auth or text) where the similarity was found to be above the minimum threshold. These files are merged to identify match candidates\n",
    "* 'results/to review/low_scores.txt' - a tab-delimited pandas dump for matches where the sum score was below the threshold for acceptance\n",
    "* 'results/to review/manual_check.txt' - a tab-delimited pandas dump for matches where a litcovid item matched with more than one preprint or vice versa\n",
    "* 'results/archives/clean_results.txt' - a tab-delimited pandas dump for matches which do not need further screening. This file is processed for creating the update dump\n",
    "* 'results/update dumps/update_file.txt' - a tab-delimited pandas dump for matches which do not need further screening and have been formatted with the appropriate fields for importing into outbreak.info resource metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pickle\n",
    "import json\n",
    "import pandas\n",
    "from pandas import read_csv\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words(\"english\")\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get the size of the source (to make it easy to figure out when to stop scrolling)\n",
    "def fetch_src_size(source):\n",
    "    pubmeta = requests.get(\"https://api.outbreak.info/resources/query?q=curatedBy.name:\"+source+\"&size=0&aggs=@type\")\n",
    "    pubjson = json.loads(pubmeta.text)\n",
    "    pubcount = int(pubjson[\"facets\"][\"@type\"][\"total\"])\n",
    "    return(pubcount)\n",
    "\n",
    "#### Pull ids from a json file\n",
    "def get_ids_from_json(jsonfile):\n",
    "    idlist = []\n",
    "    for eachhit in jsonfile[\"hits\"]:\n",
    "        if eachhit[\"_id\"] not in idlist:\n",
    "            idlist.append(eachhit[\"_id\"])\n",
    "    return(idlist)\n",
    "\n",
    "#### Ping the API and get all the ids for a specific source and scroll through the source until number of ids matches meta\n",
    "def get_source_ids(source):\n",
    "    source_size = fetch_src_size(source)\n",
    "    r = requests.get(\"https://api.outbreak.info/resources/query?q=curatedBy.name:\"+source+\"&fields=_id&fetch_all=true\")\n",
    "    response = json.loads(r.text)\n",
    "    idlist = get_ids_from_json(response)\n",
    "    try:\n",
    "        scroll_id = response[\"_scroll_id\"]\n",
    "        while len(idlist) < source_size:\n",
    "            r2 = requests.get(\"https://api.outbreak.info/resources/query?q=curatedBy.name:\"+source+\"&fields=_id&fetch_all=true&scroll_id=\"+scroll_id)\n",
    "            response2 = json.loads(r2.text)\n",
    "            idlist2 = set(get_ids_from_json(response2))\n",
    "            tmpset = set(idlist)\n",
    "            idlist = tmpset.union(idlist2)\n",
    "            try:\n",
    "                scroll_id = response2[\"_scroll_id\"]\n",
    "            except:\n",
    "                print(\"no new scroll id\")\n",
    "        return(idlist)\n",
    "    except:\n",
    "        return(idlist)\n",
    "\n",
    "#### Pull ids from the major publication sources (litcovid, medrxiv,biorxiv)\n",
    "def get_pub_ids():\n",
    "    biorxiv_ids = get_source_ids(\"biorxiv\")\n",
    "    medrxiv_ids = get_source_ids(\"medrxiv\")\n",
    "    litcovid_ids = get_source_ids(\"litcovid\")\n",
    "    preprint_ids = list(set(medrxiv_ids).union(set(biorxiv_ids)))\n",
    "    return(preprint_ids,litcovid_ids)\n",
    "\n",
    "#### Load the previously saved id lists, and compare the two to identify only the new ids\n",
    "def remove_old_ids(allidlist):\n",
    "    preprint_run = pickle.load(open(\"results/archives/all_preprint_ids.txt\", \"rb\"))\n",
    "    litcovid_run = pickle.load(open(\"results/archives/all_litcovid_ids.txt\", \"rb\"))\n",
    "    old_id_list = list(set(preprint_run).union(set(litcovid_run)))\n",
    "    new_ids_only = [x for x in allidlist if x not in old_id_list]\n",
    "    return(new_ids_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get the metadata for each list\n",
    "#### Note, I've tried batches of 1000, and the post request has failed, so this uses a batch size that's less likely to fail\n",
    "def batch_fetch_meta(idlist):\n",
    "    ## Break the list of ids into smaller chunks so the API doesn't fail the post request\n",
    "    runs = round((len(idlist))/100,0)\n",
    "    i=0 \n",
    "    separator = ','\n",
    "    ## Create dummy dataframe to store the meta data\n",
    "    textdf = pandas.DataFrame(columns = ['_id','abstract','name'])\n",
    "    authdf = pandas.DataFrame(columns = ['_id','author'])\n",
    "    while i < runs+1:\n",
    "        if len(idlist)<100:\n",
    "            sample = idlist\n",
    "        elif i == 0:\n",
    "            sample = idlist[i:(i+1)*100]\n",
    "        elif i == runs:\n",
    "            sample = idlist[i*100:len(idlist)]\n",
    "        else:\n",
    "            sample = idlist[i*100:(i+1)*100]\n",
    "        sample_ids = separator.join(sample)\n",
    "        ## Get the text-based metadata (abstract, title) and save it\n",
    "        r = requests.post(\"https://api.outbreak.info/resources/query/\", params = {'q': sample_ids, 'scopes': '_id', 'fields': 'name,abstract'})\n",
    "        if r.status_code == 200:\n",
    "            rawresult = pandas.read_json(r.text)\n",
    "            cleanresult = rawresult[['_id','name','abstract']].loc[rawresult['_score']==1].copy()\n",
    "            cleanresult.drop_duplicates(subset='_id',keep=\"first\", inplace=True)\n",
    "            textdf = pandas.concat((textdf,cleanresult))\n",
    "        ## Get the author metadata and save it    \n",
    "        a = requests.post(\"https://api.outbreak.info/resources/query/\", params = {'q': sample_ids, 'scopes': '_id', 'fields': 'author'})\n",
    "        if a.status_code == 200:\n",
    "            rawresult = pandas.read_json(a.text)\n",
    "            cleanresult = rawresult[['_id','author']].loc[rawresult['_score']==1].copy()\n",
    "            cleanresult.drop_duplicates(subset='_id',keep=\"first\", inplace=True)\n",
    "            authdf = pandas.concat((authdf,cleanresult))\n",
    "        i=i+1\n",
    "    return(textdf,authdf)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Functions for cleaning up metadata for new entries prior to running comparisons\n",
    "\n",
    "## reduce camelcase differences by lower casing everything, deal with punctuation oddities, remove stopwords and tokenize\n",
    "def text2word_tokens(section_text):\n",
    "    sample_text = section_text.lower().translate(str.maketrans('','',string.punctuation))\n",
    "    sample_set = [x for x in nltk.tokenize.word_tokenize(sample_text) if x not in stopwords]\n",
    "    return(sample_set)\n",
    "\n",
    "## Pull the ids from a dataframe\n",
    "def get_ids_from_df(rawdf_set):\n",
    "    rawdf_ids = rawdf_set['_id'].unique().tolist()\n",
    "    return(rawdf_ids)\n",
    "\n",
    "## merge title and abstract and create bag of words, remove entries missing abstract (can't be compared)\n",
    "def remove_text_na(rawdf):\n",
    "    rawdf['text'] = rawdf['name'].str.cat(rawdf['abstract'], sep=\" | \")\n",
    "    rawdf_set = rawdf.loc[~rawdf['abstract'].isna() & ~rawdf['text'].isna()].copy()\n",
    "    rawdf_set['words'] = rawdf_set.apply(lambda x: text2word_tokens(x['text']), axis=1)\n",
    "    return(rawdf_set)\n",
    "\n",
    "## create bag of words from author and remove entries missing authors (can't be compared)\n",
    "def remove_auth_na(rawdf,textset_ids):\n",
    "    rawdf_set = rawdf.loc[~rawdf['author'].isna() & rawdf['_id'].isin(textset_ids)].copy()\n",
    "    rawdf_set['author'] = rawdf_set['author'].astype(str)\n",
    "    rawdf_set['words'] = rawdf_set.apply(lambda x: text2word_tokens(x['author']), axis=1)\n",
    "    return(rawdf_set)\n",
    "\n",
    "## run the cleaning functions above on a given text dataframe, author dataframe, and source (preprint or litcovid)\n",
    "def clean_source_data(textdf,authdf,source):\n",
    "    textdf_set = remove_text_na(textdf)\n",
    "    textdf_ids = get_ids_from_df(textdf_set)\n",
    "    authdf_set = remove_auth_na(authdf,textdf_ids)\n",
    "    authdf_ids = get_ids_from_df(authdf_set)\n",
    "    return(textdf_set,authdf_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Functions for removing successful matches from old metadata prior to running comparisons\n",
    "def remove_matched_values(source,dftype):\n",
    "    clean_matches = read_csv('results/archives/clean_results.txt',delimiter='\\t',header=0,index_col=0)\n",
    "    matched_ids = clean_matches[source].unique().tolist()\n",
    "    with open(\"results/archives/\"+dftype+\"_\"+source+\"_set.txt\", \"rb\") as openfile:\n",
    "        old_source = pickle.load(openfile)\n",
    "    clean_source = old_source.loc[~old_source['_id'].isin(matched_ids)]\n",
    "    return(clean_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Pairwise jaccard index (similarity) and keep only those above a certain threshold\n",
    "Based on a test run, from when litcovid had only 30k entries and there were only slightly less than 7K preprints, text similarities >0.2 and author similarities >0.45 make a reasonable cutoff\n",
    "1. Generate bag of words for comparison and store for later use\n",
    "2. Run pairwise comparisons\n",
    "3. Merge results from pairwise comparisons based on text and authorship\n",
    "4. Sum the scores, and identify duplicates for manual checking\n",
    "5. Pull sum scores below 0.75 for manual review\n",
    "6. Pass sum scores above 0.75 for inclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Blank out the previous temp files \n",
    "def blank_temps():\n",
    "    tmppath='results/temp/'\n",
    "    tmpfiles = ['auth_above_threshold.txt','text_above_threshold.txt']\n",
    "    for eachfile in tmpfiles:\n",
    "        with open(tmppath+eachfile,'w') as outwrite:\n",
    "            outwrite.write('litcovid\\tpreprint\\tj_sim\\n')\n",
    "            \n",
    "## Run pairwise jaccard similarity calcuations and save only the results that meet the threshold into the tempfiles\n",
    "def run_comparison(preprint_set,litcovid_set,set_type, thresholds):\n",
    "    i=0\n",
    "    while i < len(litcovid_set):\n",
    "        litcovid_id = litcovid_set.iloc[i]['_id']\n",
    "        sample_set1 = litcovid_set.iloc[i]['words']\n",
    "        j=0\n",
    "        while j < len(preprint_set):\n",
    "            preprint_id = preprint_set.iloc[j]['_id']\n",
    "            sample_set2 = preprint_set.iloc[j]['words']\n",
    "            j_dist = nltk.jaccard_distance(set(sample_set1), set(sample_set2))\n",
    "            j_sim = 1-j_dist\n",
    "            if j_sim > thresholds[set_type]:\n",
    "                with open(\"results/temp/\"+set_type+\"_above_threshold.txt\",\"a\") as dump:\n",
    "                    dump.write(litcovid_id+'\\t'+preprint_id+'\\t'+str(j_sim)+'\\n')\n",
    "            j=j+1\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge the author and text matches that meet threshold, calculate sum score, and sort results\n",
    "def sort_matches(new_text_matches,new_auth_matches,threshold):\n",
    "    new_text_matches.rename(columns={'j_sim':'j_sim_text'},inplace=True)\n",
    "    new_auth_matches.rename(columns={'j_sim':'j_sim_author'},inplace=True)\n",
    "    preprint_matches = new_text_matches.merge(new_auth_matches,on=['litcovid','preprint'],how='inner')\n",
    "    preprint_matches['sum_score'] = preprint_matches['j_sim_text']+preprint_matches['j_sim_author']\n",
    "    preprint_matches['date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "    ## Set duplicates aside for manual checking\n",
    "    dupcheckdf = preprint_matches.groupby('preprint').size().reset_index(name='preprint_count')\n",
    "    dup_preprints = dupcheckdf['preprint'].loc[dupcheckdf['preprint_count']>1].tolist() ## does a preprint map to more than one pmid?\n",
    "    duplitcheckdf = preprint_matches.groupby('litcovid').size().reset_index(name='litcovid_count')\n",
    "    dup_pmids = duplitcheckdf['litcovid'].loc[duplitcheckdf['litcovid_count']>1].tolist() ## does a preprint map to more than one pmid?\n",
    "        \n",
    "    duplicates = preprint_matches.loc[(preprint_matches['litcovid'].isin(dup_pmids)) | \n",
    "                                      (preprint_matches['preprint'].isin(dup_preprints))]\n",
    "    ## Set low scores aside for manual checking\n",
    "    lowscores = preprint_matches.loc[preprint_matches['sum_score']<threshold['sum_min']]\n",
    "    ## Save the clean matches for auto updating\n",
    "    clean_matches = preprint_matches.loc[(~preprint_matches['litcovid'].isin(dup_pmids)) &\n",
    "                                         (~preprint_matches['preprint'].isin(dup_preprints)) &\n",
    "                                         (preprint_matches['sum_score']>=threshold['sum_min'])]\n",
    "\n",
    "    manual_check = pandas.concat((duplicates,lowscores),ignore_index=True)\n",
    "    return(clean_matches,lowscores,manual_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Format the results for easier updating in biothings\n",
    "def convert_txt_dumps(txtdump):\n",
    "    txtdump.rename(columns={'correction.identifier':'identifier','correction.url':'url','correction.type':'type'}, inplace=True)\n",
    "    dictlist = []\n",
    "    for i in range(len(txtdump)):\n",
    "        tmpdict={'_id':txtdump.iloc[i]['_id'],'correction':[{'@type':'Correction',\n",
    "                                                            'identifier':txtdump.iloc[i]['identifier'],\n",
    "                                                            'correctionType':txtdump.iloc[i]['type'],\n",
    "                                                            'url':txtdump.iloc[i]['url']}]}\n",
    "        dictlist.append(tmpdict)\n",
    "    return(dictlist)\n",
    "\n",
    "def generate_updates(updatedf):\n",
    "    priorupdates = read_csv('results/update dumps/update_file.tsv',delimiter=\"\\t\",header=0,index_col=0)\n",
    "    correctionA = updatedf[['litcovid','preprint']].copy()\n",
    "    correctionA.rename(columns={'litcovid':'_id','preprint':'correction.identifier'},inplace=True)\n",
    "    correctionA['correction.type']='preprint'\n",
    "    correctionA['baseurl']='https://doi.org/10.1101/'\n",
    "    correctionA['correction.url']=correctionA['baseurl'].str.cat(correctionA['correction.identifier'])\n",
    "    correctionA.drop('baseurl',axis=1,inplace=True)\n",
    "    correctionB = updatedf[['litcovid','preprint']].copy()\n",
    "    correctionB.rename(columns={'litcovid':'correction.identifier','preprint':'_id'},inplace=True)\n",
    "    correctionB['correction.type']='peer-reviewed version'\n",
    "    correctionB['baseurl']='https://pubmed.ncbi.nlm.nih.gov/'\n",
    "    correctionB['correction.url']=correctionB['baseurl'].str.cat(correctionB['correction.identifier'])\n",
    "    correctionB.drop('baseurl',axis=1,inplace=True)\n",
    "    correctionupdate = pandas.concat((priorupdates,correctionA,correctionB),ignore_index=True)\n",
    "    correctionupdate.drop_duplicates(keep='first')\n",
    "    correctionupdate.to_csv('results/update dumps/update_file.tsv',sep=\"\\t\",header=True)\n",
    "    corrections_added = len(correctionupdate)\n",
    "    json_corrections = convert_txt_dumps(correctionupdate)\n",
    "    with open('results/update dumps/update_file.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(json_corrections, f)\n",
    "    return(corrections_added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Functions for updating the save files\n",
    "\n",
    "## Update the complete ids for preprints and litcovid\n",
    "def update_archives(all_ids):\n",
    "    if 'pmid' in list(all_ids)[0]:\n",
    "        filename = 'all_litcovid_ids'\n",
    "    else:\n",
    "        filename = 'all_preprint_ids'\n",
    "    with open('results/archives/'+filename+'.txt', 'wb') as dmpfile:\n",
    "        pickle.dump(all_ids, dmpfile)\n",
    "\n",
    "## Function to update the bag of words dataframes\n",
    "def update_precompute(clean_df_set):\n",
    "    if 'pmid' in clean_df_set['_id'].iloc[0]:\n",
    "        df_source = \"litcovid\"\n",
    "    else:\n",
    "        df_source = \"preprint\"\n",
    "    if 'author' in list(clean_df_set.columns):\n",
    "        df_type = 'auth'\n",
    "    else:\n",
    "        df_type = 'text'\n",
    "    old_info = pickle.load(open(\"results/archives/\"+df_type+\"_\"+df_source+\"_set.txt\", \"rb\"))\n",
    "    updated_info = pandas.concat((old_info,clean_df_set),ignore_index=True)\n",
    "    with open(\"results/archives/\"+df_type+\"_\"+df_source+\"_set.txt\", \"wb\") as dmpfile:\n",
    "        pickle.dump(updated_info, dmpfile)\n",
    "\n",
    "## Function to update the save files for manual review or further processing (formatting for biothings)        \n",
    "def update_results(result_df):\n",
    "    update_dict = {}\n",
    "    dupcheck = result_df.groupby('litcovid').size().reset_index(name='counts')\n",
    "    dupcheck2 = result_df.groupby('preprint').size().reset_index(name='counts')\n",
    "    if len(dupcheck.loc[dupcheck['counts']>1]) or len(dupcheck2.loc[dupcheck2['counts']>1]):\n",
    "        old_manual_check = read_csv('results/to review/manual_check.txt',delimiter='\\t',header=0,index_col=0)\n",
    "        update_dict['previous matches for manual checking']=len(old_manual_check)\n",
    "        update_dict['current matches for manual checking'] =len(result_df)\n",
    "        total_manual_check = pandas.concat((old_manual_check,result_df),ignore_index=True)\n",
    "        total_manual_check.drop_duplicates(subset='_id',keep='first',inplace=True)\n",
    "        total_manual_check.to_csv('results/to review/manual_check.txt',sep='\\t',header=True)\n",
    "    elif result_df['sum_score'].max() < 0.75:\n",
    "        old_low_scores = read_csv('results/to review/low_scores.txt',delimiter='\\t',header=0,index_col=0)\n",
    "        update_dict['previous matches with low scores']=len(old_low_scores)\n",
    "        update_dict['current matches with low scores'] =len(result_df)\n",
    "        old_low_scores = pandas.concat((old_low_scores,result_df),ignore_index=True)\n",
    "        old_low_scores.drop_duplicates(subset='_id',keep='first',inplace=True)\n",
    "        old_low_scores.to_csv('results/to review/low_scores.txt',sep='\\t',header=True)\n",
    "    elif (len(dupcheck) == len(result_df)) and (len(dupcheck2)==len(result_df)):\n",
    "        old_clean_results = read_csv('results/archives/clean_results.txt',delimiter='\\t',header=0,index_col=0)\n",
    "        update_dict['previous matches for updating']=len(old_clean_results)\n",
    "        update_dict['current matches for updating'] =len(result_df)\n",
    "        old_clean_results = pandas.concat((old_clean_results,result_df),ignore_index=True)\n",
    "        old_clean_results.drop_duplicates(subset='_id',keep='first',inplace=True)\n",
    "        old_clean_results.to_csv('results/archives/clean_results.txt',sep='\\t',header=True)\n",
    "    return(update_dict)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avatar\\AppData\\Local\\Continuum\\anaconda3\\envs\\fda_spl\\lib\\site-packages\\ipykernel_launcher.py:24: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Main function\n",
    "thresholds = {\"auth\":0.45,\n",
    "              \"text\":0.2,\n",
    "              \"sum_min\":0.75}\n",
    "\n",
    "changeinfo = {'run start':datetime.now()}\n",
    "\n",
    "## pull ids\n",
    "all_preprint_ids,all_litcovid_ids = get_pub_ids()\n",
    "preprint_ids = remove_old_ids(all_preprint_ids)\n",
    "litcovid_ids = remove_old_ids(all_litcovid_ids)\n",
    "\n",
    "if len(preprint_ids) > 0:\n",
    "    update_archives(all_preprint_ids) ##update the archive file only if there are new ids\n",
    "    preprint_textdf,preprint_authdf = batch_fetch_meta(preprint_ids) ## get meta for new ids\n",
    "if len(litcovid_ids) > 0:\n",
    "    update_archives(all_litcovid_ids) ##update the archive file only if there are new ids\n",
    "    litcovid_textdf,litcovid_authdf = batch_fetch_meta(litcovid_ids) ## get meta for new ids\n",
    "    \n",
    "## log changes\n",
    "changeinfo['total litcovid ids']=len(all_litcovid_ids)\n",
    "changeinfo['total preprint ids']=len(all_preprint_ids)\n",
    "changeinfo['new litcovid ids']=len(litcovid_ids)\n",
    "changeinfo['new preprint ids']=len(preprint_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prep for comparison\n",
    "clean_lit_text,clean_lit_auth = clean_source_data(litcovid_textdf,litcovid_authdf,'litcovid')\n",
    "clean_rxiv_text,clean_rxiv_auth = clean_source_data(preprint_textdf,preprint_authdf,'preprint')\n",
    "\n",
    "## Load previous run and remove successfully mapped entries\n",
    "old_litcovid_text = remove_matched_values('litcovid','text')\n",
    "old_litcovid_auth = remove_matched_values('litcovid','auth')\n",
    "\n",
    "old_rxiv_text = remove_matched_values('preprint','auth')\n",
    "old_rxiv_auth = remove_matched_values('preprint','auth')\n",
    "\n",
    "## Clean up the temp files prior to the comparison run\n",
    "blank_temps()\n",
    "\n",
    "## run new preprints against new litcovid entries:\n",
    "if len(clean_rxiv_auth)>0 and len(clean_lit_auth)>0:\n",
    "    run_comparison(clean_rxiv_auth,clean_lit_auth,'auth', thresholds)\n",
    "if len(clean_rxiv_text)>0 and len(clean_lit_text)>0:\n",
    "    run_comparison(clean_rxiv_text,clean_lit_text,'text', thresholds)\n",
    "\n",
    "## run new litcovid entries against previous preprints\n",
    "if len(clean_lit_text)>0:\n",
    "    run_comparison(old_rxiv_text,clean_lit_text,'text', thresholds)\n",
    "if len(clean_lit_auth)>0:\n",
    "    run_comparison(old_rxiv_auth,clean_lit_auth,'auth', thresholds)\n",
    "\n",
    "## run new preprints against previously litcovid entries:\n",
    "if len(clean_rxiv_text)>0:\n",
    "    run_comparison(clean_rxiv_text,old_litcovid_text,'text', thresholds)\n",
    "if len(clean_rxiv_auth)>0:\n",
    "    run_comparison(clean_rxiv_auth,old_litcovid_auth,'auth', thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## update the set after the run\n",
    "update_precompute(clean_lit_text)\n",
    "update_precompute(clean_lit_auth)\n",
    "update_precompute(clean_rxiv_text)\n",
    "update_precompute(clean_rxiv_auth)\n",
    "\n",
    "try:\n",
    "    new_text_matches = read_csv('results/temp/text_above_threshold.txt',delimiter='\\t',header=0)\n",
    "except:\n",
    "    new_text_matches = pandas.DataFrame(columns=['litcovid','preprint','j_sim'])\n",
    "try:\n",
    "    new_auth_matches = read_csv('results/temp/auth_above_threshold.txt',delimiter='\\t',header=0)\n",
    "except:\n",
    "    new_auth_matches = pandas.DataFrame(columns=['litcovid','preprint','j_sim'])\n",
    "\n",
    "if len(new_text_matches)<1 or len(new_auth_matches)<1:\n",
    "    matchupdates = False\n",
    "else:\n",
    "    matchupdates = True\n",
    "    clean_matches,lowscores,manual_check = sort_matches(new_text_matches,new_auth_matches,thresholds)\n",
    "    \n",
    "corrections_added = generate_updates(clean_matches)\n",
    "changeinfo['new matches found']=len(clean_matches)\n",
    "changeinfo['new matches to review']=len(manual_check)\n",
    "changeinfo['new low scoring matches']=len(lowscores)\n",
    "changeinfo['new updates to make']=corrections_added\n",
    "\n",
    "manual_check_update = update_results(manual_check)\n",
    "changeinfo.update(manual_check_update)\n",
    "lowscores_update = update_results(lowscores)\n",
    "changeinfo.update(lowscores_update)\n",
    "clean_match_update = update_results(clean_matches)\n",
    "changeinfo.update(clean_match_update)\n",
    "changeinfo['run complete'] = datetime.now()\n",
    "with open('results/temp/run_log.txt','ab') as dmpfile:\n",
    "    pickle.dump(changeinfo, dmpfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_dmp = read_csv('results/update dumps/update_file.tsv', delimiter='\\t', header=0, index_col=0)\n",
    "\n",
    "dictlist = convert_txt_dumps(init_dmp)\n",
    "import json\n",
    "with open('results/update dumps/update_file.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(dictlist, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gtsueng\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
